{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Running with SLURM\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This is a long-form tutorial on SLURM. See `here <slurm-howto>` for a quick how-to.</p></div>\n\n## What is SLURM?\n\nSLURM is a job-manager used in high-performance computing (HPC) systems).\nIts purpose is to ensure the resources of the HPC are distributed fairly\nbetween users. \n\nUsers 'request' resource from SLURM to run their jobs, and are allocated\nthe resources in a priority order calculated by SLURM.\n\n## Why use SLURM?\n\nSLURM can be used to run jobs at certain computationally-heavy steps\nin the preprocessing pipeline.\n\nIn [SpikeInterface](https://spikeinterface.readthedocs.io/en/stable/),\ndata loading and preprocessing is 'lazy', meaning operations are only\nperformed on data as they are needed. This makes certain operations very fast,\nsuch as:\n\n```\nsession.preprocess(...)\nsession.plot_preprocessed(time_range=(0, 1), ...)\n```\nOnly the 1 second of data that is visualized is actually preprocessed.\n\nHowever, other operations will preprocess the entire dataset, like\n:func:`spikewrap.Session.save_preprocessed` and sorting.\n\nBecause such steps are computationally intensive, SLURM can be used to request\ndedicated resources to run the job on. It will then run in the background, allowing you\nto do other things, as well as run multiple jobs at once in parallel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running a process with SLURM\n\nIn ``spikewrap``, methods which are computationally intensive admit a ``slurm`` argument\n(currently only :func:`spikewrap.Session.save_preprocessed`).\n\n.. attention::\n    Jobs are requested at the **run level**. For example, if a\n    session has 2 runs (which are not concatenated),\n    :func:`spikewrap.Session.save_preprocessed` will request two nodes.\n\nThe ``slurm`` argument can take one of three possible inputs:\n\n``False``:\n  Do not run the job with SLURM.\n``True``:\n  Run in SLURM with a set of default parameters (explained below)\n``dict``:\n  Run in SLURM, passing the given job parameters in the dictionary to SLURM.\n\n  The arguments passed to SLURM tell it what kind of resources\n  you want to request. These are usually passed in an script using [sbatch](https://slurm.schedmd.com/sbatch.html).\n  In ``spikewrap``, [submitit](https://github.com/facebookincubator/submitit)\n  is used under the hood to submit SLURM jobs with ``sbatch``.\n\n\nIn ``spikewrap``, when ``slurm=True`` a set of default arguments are used.\nThese can be inspected with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import spikewrap as sw\nimport json\n\ndefault_arguments = sw.default_slurm_options()\n\nprint(\n    json.dumps(default_arguments, indent=4)  # json just for visualising output\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, these return arguments to request a CPU node.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Typically, HPC systems are\n    organised into a set of 'nodes' that contain isolated compute resources.\n    Nodes are requested to run jobs on. These nodes may have CPU only, or a GPU\n    (which are required by some sorters, e.g. kilosort1-3).</p></div>\n\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>These default arguments have been set up for the HPC system at the [Sainsbury Wellcome Center](https://www.sainsburywellcome.org/web/).\n   They may not all translate to other HPC systems (e.g. partition names, nodes to exclude, use of conda).\n   Please replace settings as required.</p></div>\n\nTwo of the default settings do not directly correspond to ``sbatch`` arguments:\n\n``wait``:\n   If ``True``, the job will block the executing process until the job is complete.\n\n``env_name``:\n    The conda environment in which to execute the job. By default, this will\n    read the name of the envionrment in which the script is run (from ``os.environ.get(\"CONDA_DEFAULT_ENV\")``\n    and if not found, set to ``\"spikewrap\"``.\n    To use a different environment name, edit the options as shown below.\n\n\nYou can edit these arguments before passing to the ``slurm`` argument:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpu_arguments = sw.default_slurm_options(\"gpu\")\n\ngpu_arguments[\"mem_gb\"] = 60\ngpu_arguments[\"env_name\"] = \"my_conda_environment\"\n\nprint(\n    json.dumps(gpu_arguments, indent=4)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and then use like:\n\n```\nsession.save_preprocessed(n_jobs=12, slurm=gpu_arguments)\n```\n.. admonition:: Multiprocessing in SLURM\n   :class: warning\n\n   :func:`spikewrap.Session.save_preprocessed` takes an ``n_jobs`` argument as well\n   as the ``slurm`` argument. When are using multiple cores (``n_jobs > 1``)  it is important\n   to check you have requested at least as many cpu cores as you set with ``n_jobs``.\n\n   When the SLURM job starts, ``n_jobs`` cores.  If these are not requested and available on the node,\n   the job may run slower that it should.\n\n   **Note:** ``n_jobs`` here refers to cores for correspondance with SpikeInterface terminology.\n   This is not to be confused with a SLURM job.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checking the progress of a job\n\nOnce the job has been submitted, you can track its progress\nin two ways.\n\nOne is to `inspect the SLURM output files`_.\n\nThe other is to use the commands:\n\n```\nsqueue -u my_username\n```\nTo view all your current jobs and their status.\n\n\n## Inspecting SLURM's output\n\nAs the SLURM job runs, it will output logs to a ``slurm_logs`` folder in the processed run folders.\n\nThe two main log files are the ``.out`` and ``.err`` files are written to\nthe run folder These refer to\nthe ``stdout`` and ``stderr`` streams common across the major operating systems,\nwhich manage text.\n\nOstensibly, ``stdout`` is for normal program output while and ``stderr``\nis for error messages, warnings and debugging output. However, this is not always handled\nintuitively (for example, pythons ``logging`` module will always write to ``stderr``.\nTherefore, it is important to inspect both logs while inspecting the output of the progress.\n\nThe actual outputs of the processing (e.g. preprocessed binaries)\nwill be written to the standard output folder as described [here]()\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}